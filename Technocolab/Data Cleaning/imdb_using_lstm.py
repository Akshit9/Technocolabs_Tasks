# -*- coding: utf-8 -*-
"""IMDB using LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dPi1Hn9OubM8N7666MUFiXfCf0C3txK_
"""

import tensorflow as tf
from tensorflow import keras
import tensorflow_datasets as tfds

data,data_info=tfds.load('imdb_reviews',with_info=True,as_supervised=True)

train_data,test_data=data['train'],data['test']

training_sentence=[]
training_label=[]

testing_sentence=[]
testing_label=[]

for s,l in train_data:
  training_sentence.append(s.numpy().decode('utf-8'))
  training_label.append(l.numpy())

for s,l in test_data:
  testing_sentence.append(s.numpy().decode('utf-8'))
  testing_label.append(l.numpy())

import numpy as np
final_training_labels=np.array(training_label)
final_testing_labels=np.array(testing_label)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer(num_words=10000,oov_token='<OOV>')
tokenizer.fit_on_texts(training_sentence)
word_index=tokenizer.word_index
sequences=tokenizer.texts_to_sequences(training_sentence)
padded=pad_sequences(sequences,maxlen=200,padding='post')

testing_sequences=tokenizer.texts_to_sequences(testing_sentence)
testing_padded=pad_sequences(sequences,maxlen=200,padding='post')

vocab_size=len(word_index)+1
embedding_dim=32

model=keras.Sequential([
                        keras.layers.Embedding(vocab_size,embedding_dim,input_length=200),
                        keras.layers.Bidirectional(keras.layers.LSTM(32)),
                        keras.layers.Dense(64,activation='relu'),
                        keras.layers.Dense(1,activation='sigmoid')
])

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model.summary()

history=model.fit(padded,final_training_labels,epochs=10,verbose=2,validation_data=(testing_padded,final_testing_labels))



"""Using LSTM the val_accuracy slides down

Now, we use Transfer learning
"""

import tensorflow as tf
from tensorflow import keras
import tensorflow_datasets as tfds
train_data, test_data = tfds.load(name="imdb_reviews", split=["train", "test"], 
                                  batch_size=-1, as_supervised=True)

training_sentence,training_label=tfds.as_numpy(train_data)
testing_sentence,testing_label=tfds.as_numpy(test_data)

import tensorflow_hub as hub
hub_layers=hub.KerasLayer('https://tfhub.dev/google/tf2-preview/nnlm-en-dim50-with-normalization/1',input_shape=[],trainable=True,dtype=tf.string)
pre_trained_model=keras.Sequential([
                                    hub_layers,
                                    keras.layers.Dense(32,activation='relu'),
                                    keras.layers.Dense(1,activation='sigmoid')
])

pre_trained_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

testing_size=10000
training_sentences=training_sentence[:testing_size]
training_labels=training_label[:testing_size]

testing_sentences=testing_sentence[:testing_size]
testing_labels=testing_label[:testing_size]

history=pre_trained_model.fit(training_sentences,training_labels,epochs=10,verbose=2,validation_data=(testing_sentences,testing_labels))



